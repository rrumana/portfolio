# Robots.txt for Portfolio Website
# This file controls how search engine bots crawl and index the site

# Allow all legitimate search engines to index the main content
User-agent: *
Allow: /
Allow: /index.html
Allow: /about.html
Allow: /game_of_life.html
Allow: /ReID.html
Allow: /css/
Allow: /images/
Allow: /javascript/
Allow: /scripts/
Allow: /assets/

# Disallow access to backend code and configuration
Disallow: /backend/
Disallow: /logs/
Disallow: /style/
Disallow: /wasm_game_of_life/
Disallow: /.git/
Disallow: /.gitignore
Disallow: /Cargo.toml
Disallow: /Cargo.lock
Disallow: /docker-compose.yml
Disallow: /Dockerfile
Disallow: /log4rs.yaml
Disallow: /README.md

# Disallow common sensitive files and directories
Disallow: /.env
Disallow: /.env.*
Disallow: /config/
Disallow: /tmp/
Disallow: /temp/
Disallow: /cache/
Disallow: /private/
Disallow: /admin/
Disallow: /api/

# Block aggressive crawlers and scrapers
User-agent: AhrefsBot
Disallow: /

User-agent: MJ12bot
Disallow: /

User-agent: DotBot
Disallow: /

User-agent: SemrushBot
Disallow: /

User-agent: BLEXBot
Disallow: /

User-agent: PetalBot
Disallow: /

User-agent: YandexBot
Disallow: /

User-agent: Baiduspider
Disallow: /

# Allow specific legitimate search engines
User-agent: Googlebot
Allow: /
Crawl-delay: 1

User-agent: Bingbot
Allow: /
Crawl-delay: 1

User-agent: Slurp
Allow: /
Crawl-delay: 2

User-agent: DuckDuckBot
Allow: /
Crawl-delay: 1

User-agent: facebookexternalhit
Allow: /

User-agent: Twitterbot
Allow: /

User-agent: LinkedInBot
Allow: /

# Set crawl delay for all other bots to prevent server overload
User-agent: *
Crawl-delay: 5